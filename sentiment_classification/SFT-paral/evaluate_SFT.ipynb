{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test!\n"
     ]
    }
   ],
   "source": [
    "print(\"Test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/9130/.cache/huggingface/datasets/OneFly7___parquet/OneFly7--llama2-sst2-fine-tuning-without-system-info-ff57b5730490a513/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"OneFly7/llama2-sst2-fine-tuning-without-system-info\"\n",
    "validataion_dataset = load_dataset(dataset_name, split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cb3405c1d44fe782966ff947f7dd5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "\n",
    "## Version 2-7b for finetuning\n",
    "base_model_name = \"meta-llama/Llama-2-13b-hf\"\n",
    "adapters_name = \"./models/llama-2-13b-hf-SFT-5000-1ep\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# More info: https://github.com/huggingface/transformers/pull/24906\n",
    "base_model.config.pretraining_tp = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = PeftModel.from_pretrained(base_model, adapters_name)\n",
    "# This method merges the LoRa layers into the base model. \n",
    "# This is needed if someone wants to use the base model as a standalone model.\n",
    "# peft_model = peft_model.merge_and_unload()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label_text', 'text'],\n",
       "    num_rows: 872\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validataion_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        val = self.dataset[idx]\n",
    "        label_text = val['label_text']\n",
    "        sentence = val['text']  # Here, sentence is already in the format of llama prompt tamplate\n",
    "   \n",
    "        inputs = self.tokenizer(sentence, return_tensors=\"pt\").to(\"cuda\")\n",
    "        labels = self.tokenizer(label_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': labels['input_ids'].squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'].tolist() for item in batch]\n",
    "    attention_mask = [item['attention_mask'].tolist() for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Left Padding\n",
    "    max_length = max([len(item) for item in input_ids])\n",
    "    input_ids = [[0]*(max_length - len(item)) + item for item in input_ids]\n",
    "    attention_mask = [[0]*(max_length - len(item)) + item for item in attention_mask]\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "    # Usually, labels are not padded\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_SFT(dataset, tokenizer, batch_size=16):\n",
    "    chatDataset = CustomDataset(dataset, tokenizer)\n",
    "    data_loader = DataLoader(chatDataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    peft_model.eval()\n",
    "\n",
    "    compared_result = []\n",
    "    invalid_label = []\n",
    "\n",
    "    for i, batch in enumerate(tqdm(data_loader)):\n",
    "        # Move batch to GPU\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "        labels = batch[\"labels\"].to(\"cuda\")\n",
    "\n",
    "        # Generate for the entire batch\n",
    "        outputs = peft_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=80,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode the generated text and labels\n",
    "        outputs_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        label_decoded = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        print(outputs_text[0])\n",
    "        assert 1 == 0\n",
    "\n",
    "        # Evaluate the generated text\n",
    "        for idx in range(len(outputs_text)):\n",
    "            # Extract the last sentence\n",
    "            selected_sentiment = outputs_text[idx].split(\"Sentiment: \")[1].strip()\n",
    "            selected_sentiment = selected_sentiment.split(\" \")[0].lower()\n",
    "            \n",
    "            if selected_sentiment not in ['positive', 'negative']:\n",
    "                invalid_label.append(selected_sentiment)\n",
    "                compared_result.append(0)\n",
    "                continue\n",
    "            \n",
    "            if selected_sentiment == label_decoded[idx]:\n",
    "                compared_result.append(1)\n",
    "            else:\n",
    "                compared_result.append(0)\n",
    "\n",
    "    return compared_result, invalid_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_SFT(dataset, tokenizer, batch_size=16):\n",
    "    chatDataset = CustomDataset(dataset, tokenizer)\n",
    "    data_loader = DataLoader(chatDataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    peft_model.eval()\n",
    "\n",
    "    compared_result = []\n",
    "    invalid_label = []\n",
    "\n",
    "    for i, batch in enumerate(tqdm(data_loader)):\n",
    "        # Move batch to GPU\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "        labels = batch[\"labels\"].to(\"cuda\")\n",
    "\n",
    "        # Generate for the entire batch\n",
    "        outputs = peft_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=80,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode the generated text and labels\n",
    "        outputs_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        label_decoded = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Evaluate the generated text\n",
    "        for idx in range(len(outputs_text)):\n",
    "            # Extract the last sentence\n",
    "            selected_sentiment = outputs_text[idx].split(\"Sentiment: \")[1].strip()\n",
    "            selected_sentiment = selected_sentiment.split(\" \")[1].lower()\n",
    "            \n",
    "            if selected_sentiment not in ['positive', 'negative']:\n",
    "                invalid_label.append(selected_sentiment)\n",
    "                compared_result.append(0)\n",
    "                continue\n",
    "            \n",
    "            if selected_sentiment == label_decoded[idx]:\n",
    "                compared_result.append(1)\n",
    "            else:\n",
    "                compared_result.append(0)\n",
    "\n",
    "    return compared_result, invalid_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 9/55 [09:17<47:55, 62.51s/it]"
     ]
    }
   ],
   "source": [
    "comp_res, invalid_label = evaluate_SFT(validataion_dataset, tokenizer, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def showEvalResults(compare_results, invalid_label):\n",
    "    counted_elements = Counter(invalid_label)\n",
    "    accuracy = compare_results.count(1)/len(compare_results)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"# of Invalid labels:\", len(invalid_label), \"out of\", len(compare_results), \"samples\")\n",
    "    print(\"Invalid labels:\", counted_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9128440366972477\n",
      "# of Invalid labels: 3 out of 872 samples\n",
      "Invalid labels: Counter({'[/sent]': 2, '[/sent]\\n\\nsentence:': 1})\n"
     ]
    }
   ],
   "source": [
    "showEvalResults(comp_res, invalid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
