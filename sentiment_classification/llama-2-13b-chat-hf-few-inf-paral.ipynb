{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for GPU!\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Test for GPU!\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/9130/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Found cached dataset glue (/home/9130/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Dataset\n",
    "dataset_name = \"glue\"\n",
    "task_name = \"sst2\"\n",
    "dataset = load_dataset(dataset_name, task_name, split=\"train\")\n",
    "validation = load_dataset(dataset_name, task_name, split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140cb791765244b7a925c7001b4ffa69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "\n",
    "# Version 2-13b-chat\n",
    "base_model_name = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# More info: https://github.com/huggingface/transformers/pull/24906\n",
    "base_model.config.pretraining_tp = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-13b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot Inference by Text-Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot Inference by one-Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot Inference by multi-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Define examples\n",
    "examples = [\n",
    "    {\n",
    "        \"instruction\": \"Classify the sentiment of the following text into 'positive' or 'negative':\",\n",
    "        \"sentence\": \"I love this movie!\",\n",
    "        \"label\": \"positive\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Classify the sentiment of the following text into 'positive' or 'negative':\",\n",
    "        \"sentence\": \"I am so bad today!\",\n",
    "        \"label\": \"negative\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example prompt and few-shot prompt\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"sentence\", \"label\"], \n",
    "    template=\"<s><INST> Here is an inference example:\\n{instruction}\\nSentence: {sentence}\\nSentiment: </INST> {label} </s>\",\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"<s><INST> Classify the sentiment of the following text into 'positive' or 'negative':\\nSentence: {sentence}\\nSentiment: </INST>\", \n",
    "    input_variables=[\"sentence\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><INST> Here is an inference example:\n",
      "Classify the sentiment of the following text into 'positive' or 'negative':\n",
      "Sentence: I love this movie!\n",
      "Sentiment: </INST> positive </s>\n",
      "\n",
      "<s><INST> Here is an inference example:\n",
      "Classify the sentiment of the following text into 'positive' or 'negative':\n",
      "Sentence: I am so bad today!\n",
      "Sentiment: </INST> negative </s>\n",
      "\n",
      "<s><INST> Classify the sentiment of the following text into 'positive' or 'negative':\n",
      "Sentence: I am so happy!\n",
      "Sentiment: </INST>\n"
     ]
    }
   ],
   "source": [
    "sent2test = \"I am so happy!\"\n",
    "prompt2test = prompt.format(sentence=\"I am so happy!\")\n",
    "print(prompt2test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_manual = \"\"\"\\\n",
    "<s><INST> Classify the sentiment of the following text into positive or negative:\n",
    "Here is an inference example:\n",
    "Sentence: I love this movie!\n",
    "Sentiment: </INST> positive </s>\n",
    "\n",
    "<s><INST> Here is an inference example:\n",
    "Sentence: I am so bad today!\n",
    "Sentiment: </INST> negative </s>\n",
    "\n",
    "<s><INST> Classify the sentiment of the following text into positive or negative:\n",
    "Sentence: I am so happy!\n",
    "Sentiment: </INST>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><INST> Classify the sentiment of the following text into positive or negative:\n",
      "Here is an inference example:\n",
      "Sentence: I love this movie!\n",
      "Sentiment: </INST> positive </s>\n",
      "\n",
      "<s><INST> Here is an inference example:\n",
      "Sentence: I am so bad today!\n",
      "Sentiment: </INST> negative </s>\n",
      "\n",
      "<s><INST> Classify the sentiment of the following text into positive or negative:\n",
      "Sentence: I am so happy!\n",
      "Sentiment: </INST>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s> <INST> Here is an inference example:\n",
      "Classify the sentiment of the following text into 'positive' or 'negative':\n",
      "Sentence: I love this movie!\n",
      "Sentiment: </INST> positive </s> \n",
      "\n",
      "<s> <INST> Here is an inference example:\n",
      "Classify the sentiment of the following text into 'positive' or 'negative':\n",
      "Sentence: I am so bad today!\n",
      "Sentiment: </INST> negative </s> \n",
      "\n",
      "<s> <INST> Classify the sentiment of the following text into 'positive' or 'negative':\n",
      "Sentence: I am so happy!\n",
      "Sentiment: </INST> positive</s>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt2test, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = base_model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=80, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, label_map, prompt):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_map = label_map\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        val = self.dataset[idx]\n",
    "        label_text = self.label_map[val['label']]\n",
    "        sentence = val['sentence'][:-1]\n",
    "        text = self.prompt.format(sentence=sentence)\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        return inputs, label_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Split batch into inputs and labels\n",
    "    input_ids, labels = zip(*batch)\n",
    "\n",
    "    # Compute the max length\n",
    "    # print(\"Input_ids: \", input_ids)\n",
    "    # print(\"Len(input_ids): \", len(input_ids))\n",
    "    max_length = max(ids[\"input_ids\"].shape[0] for ids in input_ids)\n",
    "\n",
    "    # Pad the sequences\n",
    "    inputs = tokenizer.pad([ids[\"input_ids\"].tolist() for ids in input_ids], \n",
    "                           padding='max_length', \n",
    "                           max_length=max_length,\n",
    "                           return_tensors=\"pt\")\n",
    "\n",
    "    return inputs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_chat(dataset, prompt):\n",
    "    label_map = {\n",
    "        0 : 'negative',\n",
    "        1 : 'positive',\n",
    "    }\n",
    "\n",
    "    batch_size = 16\n",
    "    dataset = ChatDataset(dataset, tokenizer, label_map, prompt)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    compared_result = []\n",
    "    invalid_label = []\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(tqdm(data_loader)):\n",
    "        # label_text = label_map[val['label']]\n",
    "        # sentence = val['sentence'][:-1]\n",
    "\n",
    "        # Make input\n",
    "        # text = prompt.format(sentence=sentence)\n",
    "        # inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # Generate\n",
    "        outputs = base_model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=80, pad_token_id=tokenizer.eos_token_id)\n",
    "        # print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        outputs_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # print(outputs_text)\n",
    "        selected_sentiment = outputs_text.split(\"\\n\")[-1].lower()\n",
    "        # selected_sentiment = remove_last_punctuation(selected_sentiment)\n",
    "        selected_sentiment = selected_sentiment.split(\" \")[-1]\n",
    "        # print(selected_sentiment)\n",
    "        # if i == 1:\n",
    "        #     break\n",
    "\n",
    "        # Abnormal case\n",
    "        # if selected_sentiment not in ['positive', 'negative']:\n",
    "        #     # invalid_index.append(i)\n",
    "        #     invalid_label.append(selected_sentiment)\n",
    "        #     compared_result.append(0)\n",
    "        #     continue\n",
    "\n",
    "        # Compare prediction and label\n",
    "        # assert selected_sentiment in ['positive', 'negative'], f\"Prediction {i} is not valid: {selected_sentiment}\"\n",
    "        for idx in len(batch_size):\n",
    "            if selected_sentiment[idx] not in ['positive', 'negative']:\n",
    "                invalid_label.append(selected_sentiment[idx])\n",
    "                compared_result.append(0)\n",
    "                continue\n",
    "            \n",
    "            if selected_sentiment[idx] == labels[idx]:\n",
    "                compared_result.append(1)\n",
    "            else:\n",
    "                compared_result.append(0)\n",
    "\n",
    "    return compared_result, invalid_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/55 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|          | 0/55 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m comp_res, invalid_label \u001b[39m=\u001b[39m evaluate_chat(validation, prompt)\n",
      "Cell \u001b[0;32mIn[139], line 17\u001b[0m, in \u001b[0;36mevaluate_chat\u001b[0;34m(dataset, prompt)\u001b[0m\n\u001b[1;32m     14\u001b[0m compared_result \u001b[39m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m invalid_label \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfor\u001b[39;00m i, (inputs, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(data_loader)):\n\u001b[1;32m     18\u001b[0m     \u001b[39m# label_text = label_map[val['label']]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[39m# sentence = val['sentence'][:-1]\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m     \u001b[39m# Make input\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[39m# text = prompt.format(sentence=sentence)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[39m# inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m     \u001b[39m# Generate\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     outputs \u001b[39m=\u001b[39m base_model\u001b[39m.\u001b[39mgenerate(input_ids\u001b[39m=\u001b[39minputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m), attention_mask\u001b[39m=\u001b[39minputs[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m], max_new_tokens\u001b[39m=\u001b[39m\u001b[39m80\u001b[39m, pad_token_id\u001b[39m=\u001b[39mtokenizer\u001b[39m.\u001b[39meos_token_id)\n\u001b[1;32m     27\u001b[0m     \u001b[39m# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "Cell \u001b[0;32mIn[138], line 11\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      8\u001b[0m max_length \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(ids[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m ids \u001b[39min\u001b[39;00m input_ids)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Pad the sequences\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mpad([ids[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mtolist() \u001b[39mfor\u001b[39;49;00m ids \u001b[39min\u001b[39;49;00m input_ids], \n\u001b[1;32m     12\u001b[0m                        padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m     13\u001b[0m                        max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m     14\u001b[0m                        return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     16\u001b[0m \u001b[39mreturn\u001b[39;00m inputs, labels\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2995\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   2991\u001b[0m \u001b[39m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[1;32m   2992\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m encoded_inputs:\n\u001b[1;32m   2993\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2994\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 2995\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthat includes \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, but you provided \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(encoded_inputs\u001b[39m.\u001b[39;49mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2996\u001b[0m     )\n\u001b[1;32m   2998\u001b[0m required_input \u001b[39m=\u001b[39m encoded_inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]]\n\u001b[1;32m   3000\u001b[0m \u001b[39mif\u001b[39;00m required_input \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(required_input, Sized) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(required_input) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "comp_res, invalid_label = evaluate_chat(validation, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", comp_res.count(1)/len(comp_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8314220183486238\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", comp_res.count(1)/len(comp_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(invalid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'neutral': 108, 'mixed': 7})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counted_elements = Counter(invalid_label)\n",
    "\n",
    "print(counted_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
