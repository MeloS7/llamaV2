{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for GPU!\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Test for GPU!\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/9130/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Found cached dataset glue (/home/9130/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Dataset\n",
    "dataset_name = \"glue\"\n",
    "task_name = \"sst2\"\n",
    "dataset = load_dataset(dataset_name, task_name, split=\"train\")\n",
    "validation = load_dataset(dataset_name, task_name, split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f871d008dc4ad695e73a21026abc5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "\n",
    "# Version 2-13b-chat\n",
    "base_model_name = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# More info: https://github.com/huggingface/transformers/pull/24906\n",
    "base_model.config.pretraining_tp = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-13b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot Inference by Text-Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define few-shot samples\n",
    "examples = [\n",
    "    {\n",
    "        \"instruction\": \"Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\",\n",
    "        \"sentence\": \"comes from the brave , uninhibited performances\",\n",
    "        \"label\": \"positive\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\",\n",
    "        \"sentence\": \"a depressed fifteen-year-old 's suicidal poetry\",\n",
    "        \"label\": \"negative\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "# Define example prompt and few-shot prompt\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"sentence\", \"label\"], \n",
    "    template=\"{instruction}\\nSentence: {sentence}\\nSentiment: {label}\",\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"Classify the sentiment of the following text only into these two categories :'positive' or 'negative':\\nSentence: {sentence}\\nSentiment: \", \n",
    "    input_variables=[\"sentence\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\n",
      "Sentence: comes from the brave , uninhibited performances\n",
      "Sentiment: positive\n",
      "\n",
      "Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\n",
      "Sentence: a depressed fifteen-year-old 's suicidal poetry\n",
      "Sentiment: negative\n",
      "\n",
      "Classify the sentiment of the following text only into these two categories :'positive' or 'negative':\n",
      "Sentence: I am so happy!\n",
      "Sentiment: \n"
     ]
    }
   ],
   "source": [
    "sent2test = \"I am so happy!\"\n",
    "prompt2test = prompt.format(sentence=sent2test)\n",
    "print(prompt2test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\n",
      "Sentence: comes from the brave , uninhibited performances\n",
      "Sentiment: positive\n",
      "\n",
      "Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\n",
      "Sentence: a depressed fifteen-year-old 's suicidal poetry\n",
      "Sentiment: negative\n",
      "\n",
      "Classify the sentiment of the following text only into these two categories :'positive' or 'negative':\n",
      "Sentence: I am so happy!\n",
      "Sentiment:  positive</s>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt_111, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = base_model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=80, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_text_generation(dataset, prompt, batch_size=16):\n",
    "    label_map = {\n",
    "        0 : 'negative',\n",
    "        1 : 'positive',\n",
    "    }\n",
    "\n",
    "    chatDataset = ChatDataset(dataset, tokenizer, label_map, prompt)\n",
    "    data_loader = DataLoader(chatDataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    compared_result = []\n",
    "    invalid_label = []\n",
    "\n",
    "    for i, batch in enumerate(tqdm(data_loader)):\n",
    "        # Move batch to GPU\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "        labels = batch[\"labels\"].to(\"cuda\")\n",
    "\n",
    "        # Generate for the entire batch\n",
    "        outputs = base_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=80,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode the generated text and labels\n",
    "        outputs_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        label_decoded = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # print(\"output 0\")\n",
    "        # print(outputs_text[0])\n",
    "        # print(\"output 1\")\n",
    "        # print(outputs_text[1])\n",
    "        # print(\"output 2\")\n",
    "        # print(outputs_text[2])\n",
    "        # assert 1 == 0\n",
    "\n",
    "        # Evaluate the generated text\n",
    "        for idx in range(len(outputs_text)):\n",
    "            # Extract the last sentence\n",
    "            selected_sentiment = outputs_text[idx].split(\"\\n\")[-1].lower()\n",
    "            # Remove the prompt\n",
    "            selected_sentiment = selected_sentiment.split(\" \")[-1]\n",
    "\n",
    "            # Abnormal case\n",
    "            if selected_sentiment not in ['positive', 'negative']:\n",
    "                invalid_label.append(selected_sentiment)\n",
    "                compared_result.append(0)\n",
    "                continue\n",
    "            \n",
    "            if selected_sentiment == label_decoded[idx]:\n",
    "                compared_result.append(1)\n",
    "            else:\n",
    "                compared_result.append(0)\n",
    "\n",
    "        \n",
    "\n",
    "    return compared_result, invalid_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:28<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "compared_result, invalid_label = evaluate_text_generation(validation, prompt, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9185779816513762\n",
      "# of Invalid labels: 1 out of 872 samples\n",
      "Invalid labels: Counter({'neutral': 1})\n"
     ]
    }
   ],
   "source": [
    "showEvalResults(compared_result, invalid_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot Inference by single-turn Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Define examples\n",
    "examples = [\n",
    "    {\n",
    "        \"instruction\": \"Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\",\n",
    "        \"sentence\": \"comes from the brave , uninhibited performances\",\n",
    "        \"label\": \"positive\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\",\n",
    "        \"sentence\": \"a depressed fifteen-year-old 's suicidal poetry\",\n",
    "        \"label\": \"negative\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"sentence\", \"label\"], \n",
    "    template=\"{instruction}\\nSentence: {sentence}\\nSentiment: {label}\",\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"Classify the sentiment of the following text only into these two categories :'positive' or 'negative':\\nSentence: {sentence}\\nSentiment: </INST>\", \n",
    "    input_variables=[\"sentence\"]\n",
    ")\n",
    "\n",
    "prompt_prefix = \"<s><INST> \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><INST> Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\n",
      "Sentence: comes from the brave , uninhibited performances\n",
      "Sentiment: positive\n",
      "\n",
      "Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\n",
      "Sentence: a depressed fifteen-year-old 's suicidal poetry\n",
      "Sentiment: negative\n",
      "\n",
      "Classify the sentiment of the following text only into these two categories :'positive' or 'negative':\n",
      "Sentence: I am so happy!\n",
      "Sentiment: </INST>\n"
     ]
    }
   ],
   "source": [
    "sent2test = \"I am so happy!\"\n",
    "prompt2test = prompt_prefix + prompt.format(sentence=sent2test)\n",
    "print(prompt2test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s> <INST> Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\n",
      "Sentence: comes from the brave , uninhibited performances\n",
      "Sentiment: positive\n",
      "\n",
      "Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\n",
      "Sentence: a depressed fifteen-year-old 's suicidal poetry\n",
      "Sentiment: negative\n",
      "\n",
      "Classify the sentiment of the following text only into these two categories :'positive' or 'negative':\n",
      "Sentence: I am so happy!\n",
      "Sentiment: </INST>  Sure! Here are the classifications for each sentence:\n",
      "\n",
      "1. \"comes from the brave, uninhibited performances\" - Positive\n",
      "2. \"a depressed fifteen-year-old's suicidal poetry\" - Negative\n",
      "3. \"I am so happy!\" - Positive</s>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt2test, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = base_model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=80, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot Inference by multi-turn Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Define examples\n",
    "examples = [\n",
    "    {\n",
    "        \"instruction\": \"Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\",\n",
    "        \"sentence\": \"comes from the brave , uninhibited performances\",\n",
    "        \"label\": \"positive\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\",\n",
    "        \"sentence\": \"a depressed fifteen-year-old 's suicidal poetry\",\n",
    "        \"label\": \"negative\",\n",
    "    },\n",
    "    # {\n",
    "    #     \"instruction\": \"Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\",\n",
    "    #     \"sentence\": \"it 's about issues most adults have to face in marriage and i think that 's what i liked about it -- the real issues tucked between the silly and crude storyline\",\n",
    "    #     \"label\": \"positive\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"instruction\": \"Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\",\n",
    "    #     \"sentence\": \"will find little of interest in this film , which is often preachy and poorly acted\",\n",
    "    #     \"label\": \"negative\",\n",
    "    # },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example prompt and few-shot prompt\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"sentence\", \"label\"], \n",
    "    template=\"<s><INST> {instruction}\\nSentence: {sentence}\\nSentiment: </INST> {label} </s>\",\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"<s><INST> Classify the sentiment of the following text only into these two categories :'positive' or 'negative':\\nSentence: {sentence}\\nSentiment: </INST>\", \n",
    "    input_variables=[\"sentence\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><INST> Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\n",
      "Sentence: comes from the brave , uninhibited performances\n",
      "Sentiment: </INST> positive </s>\n",
      "\n",
      "<s><INST> Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\n",
      "Sentence: a depressed fifteen-year-old 's suicidal poetry\n",
      "Sentiment: </INST> negative </s>\n",
      "\n",
      "<s><INST> Classify the sentiment of the following text only into these two categories :'positive' or 'negative':\n",
      "Sentence: I am so happy!\n",
      "Sentiment: </INST>\n"
     ]
    }
   ],
   "source": [
    "sent2test = \"I am so happy!\"\n",
    "prompt2test = prompt.format(sentence=sent2test)\n",
    "print(prompt2test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s> <INST> Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\n",
      "Sentence: comes from the brave , uninhibited performances\n",
      "Sentiment: </INST> positive </s> \n",
      "\n",
      "<s> <INST> Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\n",
      "Sentence: a depressed fifteen-year-old 's suicidal poetry\n",
      "Sentiment: </INST> negative </s> \n",
      "\n",
      "<s> <INST> Classify the sentiment of the following text only into these two categories :'positive' or 'negative':\n",
      "Sentence: I am so happy!\n",
      "Sentiment: </INST> positive</s>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt2test, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = base_model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=80, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, label_map, prompt):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_map = label_map\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        val = self.dataset[idx]\n",
    "        label_text = self.label_map[val['label']]\n",
    "        sentence = val['sentence'][:-1]\n",
    "        text = self.prompt.format(sentence=sentence)\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        labels = self.tokenizer(label_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': labels['input_ids'].squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'].tolist() for item in batch]\n",
    "    attention_mask = [item['attention_mask'].tolist() for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Left Padding\n",
    "    max_length = max([len(item) for item in input_ids])\n",
    "    input_ids = [[0]*(max_length - len(item)) + item for item in input_ids]\n",
    "    attention_mask = [[0]*(max_length - len(item)) + item for item in attention_mask]\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "    # Usually, labels are not padded\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_chat(dataset, prompt, batch_size=16):\n",
    "    label_map = {\n",
    "        0 : 'negative',\n",
    "        1 : 'positive',\n",
    "    }\n",
    "\n",
    "    chatDataset = ChatDataset(dataset, tokenizer, label_map, prompt)\n",
    "    data_loader = DataLoader(chatDataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    compared_result = []\n",
    "    invalid_label = []\n",
    "\n",
    "    for i, batch in enumerate(tqdm(data_loader)):\n",
    "        # Move batch to GPU\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "        labels = batch[\"labels\"].to(\"cuda\")\n",
    "\n",
    "        # Generate for the entire batch\n",
    "        outputs = base_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=80,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode the generated text and labels\n",
    "        outputs_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        label_decoded = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Evaluate the generated text\n",
    "        for idx in range(len(outputs_text)):\n",
    "            # Extract the last sentence\n",
    "            selected_sentiment = outputs_text[idx].split(\"\\n\")[-1].lower()\n",
    "            # Remove the prompt\n",
    "            selected_sentiment = selected_sentiment.split(\" \")[-1]\n",
    "            if selected_sentiment not in ['positive', 'negative']:\n",
    "                invalid_label.append(selected_sentiment)\n",
    "                compared_result.append(0)\n",
    "                continue\n",
    "            \n",
    "            if selected_sentiment == label_decoded[idx]:\n",
    "                compared_result.append(1)\n",
    "            else:\n",
    "                compared_result.append(0)\n",
    "\n",
    "    return compared_result, invalid_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:32<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "comp_res, invalid_label = evaluate_chat(validation, prompt, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def showEvalResults(compare_results, invalid_label):\n",
    "    counted_elements = Counter(invalid_label)\n",
    "    accuracy = compare_results.count(1)/len(compare_results)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"# of Invalid labels:\", len(invalid_label), \"out of\", len(compare_results), \"samples\")\n",
    "    print(\"Invalid labels:\", counted_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9243119266055045\n",
      "# of Invalid labels: 9 out of 872 samples\n",
      "Invalid labels: Counter({'neutral': 8, 'mixed': 1})\n"
     ]
    }
   ],
   "source": [
    "showEvalResults(comp_res, invalid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
