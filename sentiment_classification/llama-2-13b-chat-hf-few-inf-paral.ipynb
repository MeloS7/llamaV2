{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for GPU!\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Test for GPU!\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/9130/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Found cached dataset glue (/home/9130/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Dataset\n",
    "dataset_name = \"glue\"\n",
    "task_name = \"sst2\"\n",
    "dataset = load_dataset(dataset_name, task_name, split=\"train\")\n",
    "validation = load_dataset(dataset_name, task_name, split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b951c994eb446d86786a3b25fea4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "\n",
    "# Version 2-13b-chat\n",
    "base_model_name = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# More info: https://github.com/huggingface/transformers/pull/24906\n",
    "base_model.config.pretraining_tp = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-13b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot Inference by Text-Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\\\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest sentiment analysis assistant. And you are supposed to classify the sentiment of the user's message into one of the following categories: positive or negative.\n",
    "<</SYS>>\n",
    "\n",
    "Please classify the sentiment of the following message:\n",
    "Sentence: I am so happy today!\n",
    "Sentiment: [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s> [INST] <<SYS>>\n",
      "You are a helpful, respectful and honest sentiment analysis assistant. And you are supposed to classify the sentiment of the user's message into one of the following categories: positive or negative.\n",
      "<</SYS>>\n",
      "\n",
      "Please classify the sentiment of the following message:\n",
      "Sentence: I am so happy today!\n",
      "Sentiment: [/INST]\n",
      "  Sure, I'd be happy to help! Based on the sentence \"I am so happy today!\", I would classify the sentiment as POSITIVE. The use of the word \"happy\" and the exclamation mark convey a strong positive emotion.</s>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = base_model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot Inference by one-Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot Inference by multi-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Define examples\n",
    "examples = [\n",
    "    {\n",
    "        \"instruction\": \"Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\",\n",
    "        \"sentence\": \"comes from the brave , uninhibited performances\",\n",
    "        \"label\": \"positive\",\n",
    "    },\n",
    "    # {\n",
    "    #     \"instruction\": \"Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\",\n",
    "    #     \"sentence\": \"a depressed fifteen-year-old 's suicidal poetry\",\n",
    "    #     \"label\": \"negative\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"instruction\": \"Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\",\n",
    "    #     \"sentence\": \"it 's about issues most adults have to face in marriage and i think that 's what i liked about it -- the real issues tucked between the silly and crude storyline\",\n",
    "    #     \"label\": \"positive\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"instruction\": \"Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\",\n",
    "    #     \"sentence\": \"will find little of interest in this film , which is often preachy and poorly acted\",\n",
    "    #     \"label\": \"negative\",\n",
    "    # },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example prompt and few-shot prompt\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"sentence\", \"label\"], \n",
    "    template=\"<s><INST> {instruction}\\nSentence: {sentence}\\nSentiment: </INST> {label} </s>\",\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"<s><INST> Classify the sentiment of the following text only into these two categories :'positive' or 'negative':\\nSentence: {sentence}\\nSentiment: </INST>\", \n",
    "    input_variables=[\"sentence\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><INST> Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\n",
      "Sentence: comes from the brave , uninhibited performances\n",
      "Sentiment: </INST> positive </s>\n",
      "\n",
      "<s><INST> Classify the sentiment of the following text only into these two categories :'positive' or 'negative':\n",
      "Sentence: I am so happy!\n",
      "Sentiment: </INST>\n"
     ]
    }
   ],
   "source": [
    "sent2test = \"I am so happy!\"\n",
    "prompt2test = prompt.format(sentence=sent2test)\n",
    "print(prompt2test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s> <INST> Classify the sentiment of the following text only into these two categories : 'positive' or 'negative':\n",
      "Sentence: comes from the brave , uninhibited performances\n",
      "Sentiment: </INST> positive </s> \n",
      "\n",
      "<s> <INST> Classify the sentiment of the following text only into these two categories :'positive' or 'negative':\n",
      "Sentence: I am so happy!\n",
      "Sentiment: </INST> positive</s>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt2test, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = base_model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=80, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, label_map, prompt):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_map = label_map\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        val = self.dataset[idx]\n",
    "        label_text = self.label_map[val['label']]\n",
    "        sentence = val['sentence'][:-1]\n",
    "        text = self.prompt.format(sentence=sentence)\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        labels = self.tokenizer(label_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': labels['input_ids'].squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'].tolist() for item in batch]\n",
    "    attention_mask = [item['attention_mask'].tolist() for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Left Padding\n",
    "    max_length = max([len(item) for item in input_ids])\n",
    "    input_ids = [[0]*(max_length - len(item)) + item for item in input_ids]\n",
    "    attention_mask = [[0]*(max_length - len(item)) + item for item in attention_mask]\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "    # Usually, labels are not padded\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_chat(dataset, prompt, batch_size=16):\n",
    "    label_map = {\n",
    "        0 : 'negative',\n",
    "        1 : 'positive',\n",
    "    }\n",
    "\n",
    "    chatDataset = ChatDataset(dataset, tokenizer, label_map, prompt)\n",
    "    data_loader = DataLoader(chatDataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    compared_result = []\n",
    "    invalid_label = []\n",
    "\n",
    "    for i, batch in enumerate(tqdm(data_loader)):\n",
    "        # Move batch to GPU\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "        labels = batch[\"labels\"].to(\"cuda\")\n",
    "\n",
    "        # Generate for the entire batch\n",
    "        outputs = base_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=80,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode the generated text and labels\n",
    "        outputs_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        label_decoded = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Evaluate the generated text\n",
    "        for idx in range(len(outputs_text)):\n",
    "            # Extract the last sentence\n",
    "            selected_sentiment = outputs_text[idx].split(\"\\n\")[-1].lower()\n",
    "            # Remove the prompt\n",
    "            selected_sentiment = selected_sentiment.split(\" \")[-1]\n",
    "            if selected_sentiment not in ['positive', 'negative']:\n",
    "                invalid_label.append(selected_sentiment)\n",
    "                compared_result.append(0)\n",
    "                continue\n",
    "            \n",
    "            if selected_sentiment == label_decoded[idx]:\n",
    "                compared_result.append(1)\n",
    "            else:\n",
    "                compared_result.append(0)\n",
    "\n",
    "    return compared_result, invalid_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:55<00:00,  1.99s/it]\n"
     ]
    }
   ],
   "source": [
    "comp_res, invalid_label = evaluate_chat(validation, prompt, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8256880733944955\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", comp_res.count(1)/len(comp_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(invalid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter()\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counted_elements = Counter(invalid_label)\n",
    "\n",
    "print(counted_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
