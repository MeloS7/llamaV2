{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Scucessfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Scucessfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/9130/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Found cached dataset glue (/home/9130/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Dataset\n",
    "dataset_name = \"glue\"\n",
    "task_name = \"sst2\"\n",
    "dataset = load_dataset(dataset_name, task_name, split=\"train\")\n",
    "validation = load_dataset(dataset_name, task_name, split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089bcdce2eb64bb2888c0e40b62c18fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "\n",
    "# Version 2-7b\n",
    "# base_model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# Version 2-13b-chat\n",
    "base_model_name = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# More info: https://github.com/huggingface/transformers/pull/24906\n",
    "base_model.config.pretraining_tp = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-13b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHAT INFERENCE MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogs = \"\"\"\\\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest sentiment analysis assistant. And you are supposed to classify the sentiment of the user's message into one of the following categories: positive or negative.\n",
    "<</SYS>>\n",
    "\n",
    "Here is an inference example:\n",
    "Sentence: I am so happy today.\n",
    "Sentiment: [/INST] positive </s><s>[INST] Here is another inference example:\n",
    "Sentence: I am so sad today.\n",
    "Sentiment: [/INST] negative </s><s>[INST] Please classify the sentiment of the following sentence into positive or negative:\n",
    "Sentence: it 's a charming and often affecting journey .\n",
    "Sentiment: [/INST] \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest sentiment analysis assistant. And you are supposed to classify the sentiment of the user's message into one of the following categories: positive or negative.\n",
      "<</SYS>>\n",
      "\n",
      "Here is an inference example:\n",
      "Sentence: I am so happy today.\n",
      "Sentiment: [/INST] positive  [INST] Here is another inference example:\n",
      "Sentence: I am so sad today.\n",
      "Sentiment: [/INST] negative  [INST] Please classify the sentiment of the following sentence into positive or negative:\n",
      "Sentence: it 's a charming and often affecting journey .\n",
      "Sentiment: [/INST] \n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(dialogs, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = base_model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Inference Mode 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> You are a helpful, respectful and honest sentiment analysis assistant. And you are supposed to classify the sentiment of the user's message into one of the following categories: positive or negative.\n",
      "Here is an inference example:\n",
      "Sentence: I am so happy today.\n",
      "Sentiment: positive\n",
      "\n",
      "Here is another inference example:\n",
      "Sentence: I am so sad today.\n",
      "Sentiment: negative\n",
      "\n",
      "Sentence: It really sucks that I have to do this.\n",
      "Sentiment:  negative\n",
      "\n",
      "Sentence: I am so excited to go on vacation.\n",
      "Sentiment: positive\n",
      "\n",
      "Now, please classify the sentiment of the following message:\n",
      "\n",
      "Sentence: I am so frustrated with this stupid app.\n",
      "\n",
      "Please select one of the following options:\n",
      "\n",
      "A) positive\n",
      "B) negative\n",
      "\n",
      "Please select one of the options based on the sentiment of the message.</s>\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful, respectful and honest sentiment analysis assistant. And you are supposed to classify the sentiment of the user's message into one of the following categories: positive or negative.\"\n",
    "sentence = \"It really sucks that I have to do this.\"\n",
    "examples = \"\"\"\n",
    "Here is an inference example:\n",
    "Sentence: I am so happy today.\n",
    "Sentiment: positive\n",
    "\n",
    "Here is another inference example:\n",
    "Sentence: I am so sad today.\n",
    "Sentiment: negative\n",
    "\"\"\"\n",
    "\n",
    "# text = system_prompt + \"\\n\" + user_msg_1 + \"\\n\" + \"Sentiment:\"\n",
    "text = system_prompt + examples + \"\\n\" + \"Sentence: \" + sentence + \"\\n\" + \"Sentiment: \"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = base_model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Mode 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 110, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful, respectful and honest sentiment analysis assistant. And you are supposed to classify the sentiment of the user's message into one of the following categories: positive or negative.\n",
      "Here is an inference example:\n",
      "Sentence: I am so happy today.\n",
      "Sentiment: positive\n",
      "\n",
      "Here is another inference example:\n",
      "Sentence: I am so sad today.\n",
      "Sentiment: negative\n",
      "\n",
      "\n",
      "Sentence: unflinchingly bleak and desperate\n",
      "Sentiment:  negative\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful, respectful and honest sentiment analysis assistant. And you are supposed to classify the sentiment of the user's message into one of the following categories: positive or negative.\"\n",
    "sentence = \"unflinchingly bleak and desperate\"\n",
    "\n",
    "# text = system_prompt + \"\\n\" + user_msg_1 + \"\\n\" + \"Sentiment:\"\n",
    "text = system_prompt + examples + \"\\n\" + \"Sentence: \" + sentence + \"\\n\" + \"Sentiment: \"\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=tokenizer, max_length=100)\n",
    "result = pipe(f\"{text}\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Inference with text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt\n",
    "system_prompt = \"You are a helpful, respectful and honest sentiment analysis assistant. And you are supposed to classify the sentiment of the user's message into one of the following categories: positive or negative.\"\n",
    "# User Prompt\n",
    "user_prompt = \"Please classify the sentiment of the following sentence into positive or negative:\"\n",
    "# Examples\n",
    "examples = \"\"\"\n",
    "Here is an inference example:\n",
    "Sentence: I am so happy today.\n",
    "Sentiment: positive\n",
    "\n",
    "Here is another inference example:\n",
    "Sentence: I am so sad today.\n",
    "Sentiment: negative\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(dataset):\n",
    "    label_map = {\n",
    "        0 : 'negative',\n",
    "        1 : 'positive',\n",
    "    }\n",
    "\n",
    "    compared_result = []\n",
    "\n",
    "    for i, val in enumerate(tqdm(dataset)):\n",
    "        label_text = label_map[val['label']]\n",
    "        sentence = val['sentence']\n",
    "\n",
    "        # Make input\n",
    "        text = user_prompt + examples + \"\\n\" + \"Sentence: \" + sentence + \"\\n\" + \"Sentiment: \"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # Generate\n",
    "        outputs = base_model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=80, pad_token_id=tokenizer.eos_token_id)\n",
    "        # print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        outputs_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        print(outputs_text)\n",
    "        if i == 2:\n",
    "            break\n",
    "\n",
    "        matches = re.findall(r\"Sentiment: (.+)\", outputs_text)\n",
    "        selected_sentiment = matches[2].strip() if len(matches) >= 3 else None\n",
    "        # print(selected_sentiment)\n",
    "\n",
    "        # Compare prediction and label\n",
    "        if selected_sentiment == label_text:\n",
    "            compared_result.append(1)\n",
    "        else:\n",
    "            compared_result.append(0)\n",
    "\n",
    "    return compared_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/872 [00:00<03:42,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please classify the sentiment of the following sentence into positive or negative:\n",
      "Here is an inference example:\n",
      "Sentence: I am so happy today.\n",
      "Sentiment: positive\n",
      "\n",
      "Here is another inference example:\n",
      "Sentence: I am so sad today.\n",
      "Sentiment: negative\n",
      "\n",
      "\n",
      "Sentence: it 's a charming and often affecting journey . \n",
      "Sentiment:  positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/872 [00:04<40:03,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please classify the sentiment of the following sentence into positive or negative:\n",
      "Here is an inference example:\n",
      "Sentence: I am so happy today.\n",
      "Sentiment: positive\n",
      "\n",
      "Here is another inference example:\n",
      "Sentence: I am so sad today.\n",
      "Sentiment: negative\n",
      "\n",
      "\n",
      "Sentence: unflinchingly bleak and desperate \n",
      "Sentiment:  negative\n",
      "\n",
      "Sentence: unflinchingly hopeful and optimistic \n",
      "Sentiment: positive\n",
      "\n",
      "Please classify the sentiment of the following sentence into positive or negative:\n",
      "Sentence: unflinchingly bleak and desperate.\n",
      "\n",
      "Please select one of the following options:\n",
      "\n",
      "A) positive\n",
      "B) negative\n",
      "C) neutral\n",
      "\n",
      "Please\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/872 [00:05<36:22,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please classify the sentiment of the following sentence into positive or negative:\n",
      "Here is an inference example:\n",
      "Sentence: I am so happy today.\n",
      "Sentiment: positive\n",
      "\n",
      "Here is another inference example:\n",
      "Sentence: I am so sad today.\n",
      "Sentiment: negative\n",
      "\n",
      "\n",
      "Sentence: allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker . \n",
      "Sentiment:  positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_list = evaluate(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8623853211009175\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", result_list.count(1)/len(result_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot Inference with chat-generation (LangChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Here is an inference example:\n",
      "Sentence: I am so happy today.\n",
      "Sentiment: [/INST] positive </s><s>[INST] Here is another inference example:\n",
      "Sentence: I am so sad today.\n",
      "Sentiment: [/INST] negative </s>\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Define the system prompt\n",
    "system_prompt_template = \"<<SYS>>\\n{system_prompt_text}\\n<</SYS>>\\n\\n\"\n",
    "system_prompt_text = \"You are a helpful, respectful and honest sentiment analysis assistant. And you are supposed to classify the sentiment of the user's message into one of the following categories: 'positive' or 'negative'.\"\n",
    "system_prompt = system_prompt_template.format(system_prompt_text=system_prompt_text)\n",
    "\n",
    "# define few-shot inference instructions\n",
    "inf_instruction1 = \"Here is an inference example:\\n\"\n",
    "inf_instruction2 = \"Here is another inference example:\\n\"\n",
    "\n",
    "# Define the example prompts\n",
    "example_prompts = [\n",
    "    {\"system_prompt\": inf_instruction1, \"sentence\": \"I am so happy today.\", \"sentiment\": \"positive\"},\n",
    "    # {\"system_prompt\": inf_instruction2, \"sentence\": \"They are really friendly.\", \"sentiment\": \"positive\"},\n",
    "    {\"system_prompt\": inf_instruction2, \"sentence\": \"I am so sad today.\", \"sentiment\": \"negative\"},\n",
    "    # {\"system_prompt\": inf_instruction2, \"sentence\": \"I hate spiders, they are creepy and scary.\", \"sentiment\": \"negative\"}\n",
    "]\n",
    "\n",
    "# Define the format of the instance prompt\n",
    "inst_format = \"<s>[INST] {system_prompt}Sentence: {sentence}\\nSentiment: [/INST] {sentiment} </s>\"\n",
    "\n",
    "# Define the format of the inference prompt\n",
    "inference_format = \"<s>[INST] {system_prompt}Sentence: {sentence}\\nSentiment: [/INST] \\n\"\n",
    "\n",
    "# Start constructing the prompts\n",
    "# prompts = \"<s>[INST] <<SYS>>\\n\" + system_prompt + \"\\n<</SYS>>\\n\\n\"\n",
    "prompts = \"\"\n",
    "\n",
    "# Add the example prompts\n",
    "for example in example_prompts:\n",
    "    prompts += inst_format.format(**example)\n",
    "\n",
    "# Your final prompts look like this:\n",
    "print(prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_prompt = \"Please classify the sentiment of the following sentence into one of the following categories:'positive' or 'negative':\\n\"\n",
    "# inference_prompt = \"Please tell me the following sentence is positive or negative:\\n\"\n",
    "# There is a space at the end of the sst2 sentence, so we need to remove it\n",
    "data = {\"system_prompt\": inference_prompt, \"sentence\": validation[20]['sentence'][:-1], \"sentiment\": \"\"}\n",
    "try_p = prompts + inference_format.format(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Here is an inference example:\n",
      "Sentence: I am so happy today.\n",
      "Sentiment: [/INST] positive </s><s>[INST] Here is another inference example:\n",
      "Sentence: I am so sad today.\n",
      "Sentiment: [/INST] negative </s><s>[INST] Please classify the sentiment of the following sentence into one of the following categories:'positive' or 'negative':\n",
      "Sentence: pumpkin takes an admirable look at the hypocrisy of political correctness , but it does so with such an uneven tone that you never know when humor ends and tragedy begins .\n",
      "Sentiment: [/INST] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(try_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_last_punctuation(text):\n",
    "    punctuation_chars = set(string.punctuation)\n",
    "\n",
    "    selected_sentiment = text.split(\"\\n\")[-1].lower()\n",
    "\n",
    "    # If the last character is a punctuation, remove it\n",
    "    while selected_sentiment and selected_sentiment[-1] in punctuation_chars:\n",
    "        selected_sentiment = selected_sentiment[:-1]\n",
    "\n",
    "    return selected_sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_chat(dataset, prefix_prompt, inference_format, user_prompt):\n",
    "    label_map = {\n",
    "        0 : 'negative',\n",
    "        1 : 'positive',\n",
    "    }\n",
    "\n",
    "    compared_result = []\n",
    "    invalid_index = []\n",
    "\n",
    "    for i, val in enumerate(tqdm(dataset)):\n",
    "        label_text = label_map[val['label']]\n",
    "        sentence = val['sentence'][:-1]\n",
    "        sent_dict = {\n",
    "            \"system_prompt\": user_prompt,\n",
    "            \"sentence\": sentence,\n",
    "            \"sentiment\": \"\"\n",
    "        }\n",
    "\n",
    "        # Make input\n",
    "        text = prefix_prompt + inference_format.format(**sent_dict)\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # Generate\n",
    "        outputs = base_model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=80, pad_token_id=tokenizer.eos_token_id)\n",
    "        # print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        outputs_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # print(outputs_text)\n",
    "        selected_sentiment = outputs_text.split(\"\\n\")[-1].lower()\n",
    "        selected_sentiment = remove_last_punctuation(selected_sentiment)\n",
    "        # print(selected_sentiment)\n",
    "        # if i == 21:\n",
    "        #     break\n",
    "\n",
    "        # Abnormal case\n",
    "        if selected_sentiment not in ['positive', 'negative']:\n",
    "            invalid_index.append(i)\n",
    "            compared_result.append(0)\n",
    "            continue\n",
    "\n",
    "        # Compare prediction and label\n",
    "        # assert selected_sentiment in ['positive', 'negative'], f\"Prediction {i} is not valid: {selected_sentiment}\"\n",
    "        if selected_sentiment == label_text:\n",
    "            compared_result.append(1)\n",
    "        else:\n",
    "            compared_result.append(0)\n",
    "\n",
    "    return compared_result, invalid_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/872 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 872/872 [09:03<00:00,  1.60it/s]\n"
     ]
    }
   ],
   "source": [
    "res_chat, invalid_index = evaluate_chat(validation, prompts, inference_format, inference_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8222477064220184\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", res_chat.count(1)/len(res_chat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "872"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(invalid_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
