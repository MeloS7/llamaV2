{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Scucessfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Scucessfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/9130/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Found cached dataset glue (/home/9130/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Dataset\n",
    "dataset_name = \"glue\"\n",
    "task_name = \"sst2\"\n",
    "dataset = load_dataset(dataset_name, task_name, split=\"train\")\n",
    "validation = load_dataset(dataset_name, task_name, split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef573ccf0124da192fbda0c4cdda139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "\n",
    "# Version 2-7b\n",
    "# base_model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# Version 2-13b-chat\n",
    "# base_model_name = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "# Ver\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# More info: https://github.com/huggingface/transformers/pull/24906\n",
    "base_model.config.pretraining_tp = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-13b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Mode 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful, respectful and honest sentiment analysis assistant. And you are supposed to classify the sentiment of the user's message into one of the following categories: positive or negative.\n",
      "I am so happy today.\n",
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful, respectful and honest sentiment analysis assistant. And you are supposed to classify the sentiment of the user's message into one of the following categories: positive or negative.\"\n",
    "user_msg_1 = \"I am so happy today.\"\n",
    "\n",
    "text = system_prompt + \"\\n\" + user_msg_1 + \"\\n\" + \"Sentiment:\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = base_model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Mode 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a llama?\n",
      "\n",
      "A llama is a large, domesticated South American mammal that is closely related to the camel. It is known for its distinctive long neck, ears, and legs, as well as its soft, woolly fur. Llamas are often used as pack animals, and are also kept as pets and show animals. They are native to the Andean regions of South America, and have been bred by humans for thousands of years.\n",
      "\n",
      "\n",
      "\n",
      "What is a llama used for?\n",
      "\n",
      "Llamas are used for a variety of purposes, including:\n",
      "\n",
      "\n",
      "\n",
      "1. Packing: Llamas are often used as pack animals, carrying goods and supplies over long distances.\n",
      "\n",
      "2. Transportation: Llamas can be ridden like horses, and are sometimes used as a mode of transportation in rural areas.\n",
      "\n",
      "3. Agriculture: Llamas are used\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is a llama?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"{prompt}\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt\n",
    "system_prompt = \"You are a helpful, respectful and honest sentiment analysis assistant. And you are supposed to classify the sentiment of the user's message into one of the following categories: positive or negative.\"\n",
    "# User Prompt\n",
    "user_prompt = \"Please classify the sentiment of the following sentence into positive or negative:\"\n",
    "# Examples\n",
    "examples = \"\"\"\n",
    "Here is an inference example:\n",
    "Sentence: I am so happy today.\n",
    "Sentiment: positive\n",
    "\n",
    "Here is another inference example:\n",
    "Sentence: I am so sad today.\n",
    "Sentiment: negative\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(dataset):\n",
    "    label_map = {\n",
    "        0 : 'negative',\n",
    "        1 : 'positive',\n",
    "    }\n",
    "\n",
    "    compared_result = []\n",
    "\n",
    "    for i, val in enumerate(tqdm(dataset)):\n",
    "        label_text = label_map[val['label']]\n",
    "        sentence = val['sentence']\n",
    "\n",
    "        # Make input\n",
    "        text = user_prompt + examples + \"\\n\" + \"Sentence: \" + sentence + \"\\n\" + \"Sentiment: \"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # Generate\n",
    "        outputs = base_model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=80, pad_token_id=tokenizer.eos_token_id)\n",
    "        # print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        outputs_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        matches = re.findall(r\"Sentiment: (.+)\", outputs_text)\n",
    "        selected_sentiment = matches[2].strip() if len(matches) >= 3 else None\n",
    "        # print(selected_sentiment)\n",
    "\n",
    "        # Compare prediction and label\n",
    "        if selected_sentiment == label_text:\n",
    "            compared_result.append(1)\n",
    "        else:\n",
    "            compared_result.append(0)\n",
    "\n",
    "    return compared_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 872/872 [14:06<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "result_list = evaluate(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8623853211009175\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", result_list.count(1)/len(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
